# spark cookbook
## 一. 安装与配置

1. 安装与配置
  > 参考1
	- 安装配置jdk 8(https://download.oracle.com/otn-pub/java/jdk/8u201-b09/42970487e3af4f5aa5bca3f542482c60/jdk-8u201-windows-x64.exe)， 配置JAVA_HOME、CLASSPATH、Path环境变量
	- 安装配置[scala](https://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.msi)， 在环境变量Path中，添加scala的bin文件夹
	- 安装配置[spark](http://mirror.bit.edu.cn/apache/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz), 解压、添加环境变量SPARK_HOME，并在Path添加spark的bin文件夹
	- 安装配置[Hadoop](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1)
	- 下载winutils
	-配置pyspark

2. 在Jupyter或者Spyder中运行pyspark
	
	加入如下代码即可:	
	
```
import os
import sys

spark_home = os.environ.get('SPARK_HOME', None)
if not spark_home:
    raise ValueError('SPARK_HOME environment variable is not set')
sys.path.insert(0, os.path.join(spark_home, 'python'))
sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.4-src.zip'))
comm=os.path.join(spark_home, 'python/lib/py4j-0.10.4-src.zip')
print ('start spark....',comm)
exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())
```

## PySpark体系

Apache spark允许用户读取、转换、聚合数据，还可以训练和部署复杂的统计模型。
1.	Spark dataframe
2.	Spark sql
3.	Spark MLlib、Spark ML
4.	GraphX、GraphFrames
5.	Spark Streaming(Dstream\Structured)


### 一.	RDD 弹性分布式数据集
1.	创建rdd
a)	parallelize
b)	sc.textfile
2.	转换
a)	map， 执行在每一个RDD元素上
b)	filter， 条件过滤，执行在每一个RDD元素上
c)	flatMap， 和map类似，但是返回一个扁平的结果，而不是列表
d)	distinct， 返回指定列中不同值
e)	sample， 返回随机样本
f)	leftOuterJoin， 左连接， 1对多则返回1对多条
g)	join， 内连接
h)	repartition， 改变数据集分区数量
3.	操作
a)	take
b)	collect
c)	reduce
d)	reduceByKey， 按照键值，进行reduce操作
e)	count， 返回RDD元素个数
f)	saveAsTextFile， 将RDD保存为文本文件
g)	foreach
转换是惰性的，操作才是实时的！

### 二.	DataFrame

DataFrame是不可变的分布式数据集！

1.	创建DataFrame
SwimmersJSON = spark.read.json(stringJSONRDD) #将RDD转化为DataFrame
Swimmers = spark.createDataFrame(stringCSVRDD, schema) #可以指定schema，即每列类型等
2.	DataFrame查询
a)	API查询， swimmersJSON.show()
b)	SQL查询， spark.sql(“SELECT * FROM swimmersJSON”).collect()
 

	API	SQL
行数	swimmers.count()	Swimmers.sql(“SELECT COUNT(*) FROM swimmers”).show()
筛选语句	swimmers.select(“id”, “age”).filter(“age=22”).show()
	Swimmers.sql(“SELECT id, age FROM swimmers WHERE age=22”).show()


### 三.	Spark数据清理
1.	数据查重
df.count(), df.distinct().count()
dropDuplicates()

• 所有字段均相等
• 除主键外，所有字段均相等
• 是否有重复的ID import pyspark.sql.functions as fn
2.	空值检验与处理
a)	样本
b)	属性
3.	异常值检验
approxQuantile()

### 四.	数据描述性统计
1.	描述性统计
describe(输出count、mean、stddev、min、max)
df.agg({‘var’:’func’})， 聚合函数func包括avg\count\first\kurtosis\skewness\max\min…
2.	相关性
df.corr
3.	可视化
bokeh、matplotlib

### 五.	MLlib
### 六.	ML包
### 七.	GraphFrames
### 八.	TensorFrames
### 九.	Spark Streaming

## 参考
1. [spark安装与配置](https://www.cnblogs.com/momogua/p/9285930.html)
2. [spark blog](http://www.cnblogs.com/shishanyuan/p/4699644.html) 
3. [spark算子](http://lxw1234.com/archives/2015/07/363.htm)
4. [pyspark算子](https://blog.csdn.net/kittyzc/article/details/81739008)
